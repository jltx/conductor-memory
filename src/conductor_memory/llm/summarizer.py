"""
File summarizer for background LLM summarization.

Handles file analysis, skeleton extraction for large files, and LLM prompt generation.
"""

import json
import logging
import re
from typing import Dict, Any, Optional, List, Union, Tuple
from dataclasses import dataclass
from pathlib import Path

from .base import LLMClient, LLMResponse, LLMError
from ..search.heuristics import HeuristicMetadata
from ..config.summarization import SummarizationConfig

logger = logging.getLogger(__name__)


@dataclass
class FileSummary:
    """Summary of a file generated by LLM."""
    file_path: str
    language: str
    purpose: str
    pattern: str
    key_exports: List[str]
    dependencies: List[str]
    domain: str
    model_used: str
    tokens_used: Optional[int] = None
    response_time_ms: Optional[float] = None
    is_skeleton: bool = False
    error: Optional[str] = None
    # Phase 2: Implementation-aware summary fields
    how_it_works: Optional[str] = None  # Explanation of HOW the code works
    key_mechanisms: Optional[List[str]] = None  # Key mechanisms/patterns used
    method_summaries: Optional[Dict[str, str]] = None  # Per-method summaries
    # Simple file detection fields
    simple_file: bool = False  # True if summarized without LLM
    simple_file_reason: Optional[str] = None  # Reason for simple classification
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        result = {
            'file_path': self.file_path,
            'language': self.language,
            'purpose': self.purpose,
            'pattern': self.pattern,
            'key_exports': self.key_exports,
            'dependencies': self.dependencies,
            'domain': self.domain,
            'model_used': self.model_used,
            'tokens_used': self.tokens_used,
            'response_time_ms': self.response_time_ms,
            'is_skeleton': self.is_skeleton,
            'error': self.error,
            'simple_file': self.simple_file,
            'simple_file_reason': self.simple_file_reason
        }
        # Include implementation-aware fields when present
        if self.how_it_works is not None:
            result['how_it_works'] = self.how_it_works
        if self.key_mechanisms is not None:
            result['key_mechanisms'] = self.key_mechanisms
        if self.method_summaries is not None:
            result['method_summaries'] = self.method_summaries
        return result


class FileSummarizer:
    """Summarizes files using LLM with skeleton extraction for large files."""
    
    # Template summaries for simple files (no LLM needed)
    SIMPLE_TEMPLATES = {
        "empty_module": {
            "purpose": "Empty module placeholder",
            "pattern": "Empty",
            "domain": "infrastructure"
        },
        "barrel_reexport": {
            "purpose": "Re-exports {count} symbols from submodules: {modules}",
            "pattern": "Barrel",
            "domain": "infrastructure"
        },
        "type_definitions": {
            "purpose": "Type definitions for {domain}",
            "pattern": "Types",
            "domain": "types"
        },
        "constants_only": {
            "purpose": "Configuration constants",
            "pattern": "Constants",
            "domain": "configuration"
        },
        "generated_code": {
            "purpose": "Auto-generated code (do not edit manually)",
            "pattern": "Generated",
            "domain": "generated"
        }
    }
    
    def __init__(self, llm_client: LLMClient, config: SummarizationConfig):
        """
        Initialize file summarizer.
        
        Args:
            llm_client: LLM client to use for summarization
            config: Summarization configuration
        """
        self.llm_client = llm_client
        self.config = config
    
    async def summarize_file(
        self, 
        file_path: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata] = None
    ) -> FileSummary:
        """
        Summarize a file using LLM or template (for simple files).
        
        Args:
            file_path: Path to the file
            content: File content
            heuristic_metadata: Optional heuristic metadata for context
            
        Returns:
            FileSummary with LLM-generated or template-based summary
        """
        try:
            # Determine language
            language = self._detect_language(file_path, heuristic_metadata)
            
            # Check if file should be skipped
            if self._should_skip_file(file_path):
                return FileSummary(
                    file_path=file_path,
                    language=language,
                    purpose="Skipped file (matches skip pattern)",
                    pattern="Skipped",
                    key_exports=[],
                    dependencies=[],
                    domain="skipped",
                    model_used="none",
                    error="File skipped due to skip pattern"
                )
            
            # Check if this is a simple file that doesn't need LLM summarization
            # Only use simple file detection when enabled in config
            if (self.config.enable_simple_file_detection and 
                heuristic_metadata and heuristic_metadata.is_simple_file):
                return self._generate_simple_summary(
                    file_path, content, language, heuristic_metadata
                )
            
            # Determine if we need to extract skeleton
            lines = content.split('\n')
            estimated_tokens = len(content.split())  # Rough token estimate
            
            use_skeleton = (
                len(lines) > self.config.max_file_lines or 
                estimated_tokens > self.config.max_file_tokens
            )
            
            # Prepare content for LLM
            if use_skeleton:
                llm_content = self._extract_skeleton(content, language, heuristic_metadata)
            else:
                llm_content = content
            
            # Generate summary using LLM
            summary_data = await self._generate_llm_summary(
                file_path, language, llm_content, heuristic_metadata
            )
            
            return FileSummary(
                file_path=file_path,
                language=language,
                purpose=summary_data.get('purpose', 'Unknown purpose'),
                pattern=summary_data.get('pattern', 'Unknown'),
                key_exports=summary_data.get('key_exports', []),
                dependencies=summary_data.get('dependencies', []),
                domain=summary_data.get('domain', 'unknown'),
                model_used=summary_data.get('model_used', 'unknown'),
                tokens_used=summary_data.get('tokens_used'),
                response_time_ms=summary_data.get('response_time_ms'),
                is_skeleton=use_skeleton,
                # Phase 2: Implementation-aware fields (optional, populated when LLM provides them)
                how_it_works=summary_data.get('how_it_works'),
                key_mechanisms=summary_data.get('key_mechanisms'),
                method_summaries=summary_data.get('method_summaries'),
                simple_file=False,
                simple_file_reason=None
            )
            
        except Exception as e:
            logger.warning(f"Failed to summarize {file_path}: {e}")
            return FileSummary(
                file_path=file_path,
                language=self._detect_language(file_path, heuristic_metadata),
                purpose="Failed to generate summary",
                pattern="Error",
                key_exports=[],
                dependencies=[],
                domain="error",
                model_used="none",
                error=str(e)
            )
    
    def _detect_language(self, file_path: str, heuristic_metadata: Optional[HeuristicMetadata]) -> str:
        """Detect programming language from file path or metadata."""
        if heuristic_metadata:
            return heuristic_metadata.language
        
        # Fallback to file extension
        ext = Path(file_path).suffix.lower()
        ext_to_lang = {
            '.py': 'python',
            '.java': 'java',
            '.kt': 'kotlin',
            '.go': 'go',
            '.cs': 'csharp',
            '.swift': 'swift',
            '.m': 'objc',
            '.rb': 'ruby',
            '.c': 'c',
            '.h': 'c',
            '.cpp': 'cpp',
            '.hpp': 'cpp'
        }
        return ext_to_lang.get(ext, 'unknown')
    
    def _should_skip_file(self, file_path: str) -> bool:
        """Check if file should be skipped based on patterns."""
        import fnmatch
        
        for pattern in self.config.skip_patterns:
            if fnmatch.fnmatch(file_path, pattern):
                return True
        return False
    
    def _generate_simple_summary(
        self,
        file_path: str,
        content: str,
        language: str,
        heuristic_metadata: HeuristicMetadata
    ) -> FileSummary:
        """
        Generate a template-based summary for simple files (no LLM call needed).
        
        This is significantly faster than LLM summarization (<100ms vs 2-5s) and
        produces consistent summaries for common patterns like barrel files,
        empty modules, and generated code.
        
        Args:
            file_path: Path to the file
            content: File content
            language: Detected programming language
            heuristic_metadata: Heuristic metadata with is_simple_file and simple_file_reason
            
        Returns:
            FileSummary with template-based summary
        """
        import time
        start_time = time.time()
        
        reason = heuristic_metadata.simple_file_reason or "empty_module"
        template = self.SIMPLE_TEMPLATES.get(reason, self.SIMPLE_TEMPLATES["empty_module"])
        
        # Initialize with template defaults
        purpose = template["purpose"]
        pattern = template["pattern"]
        domain = template["domain"]
        key_exports: List[str] = []
        dependencies: List[str] = []
        
        # Customize based on reason
        if reason == "barrel_reexport":
            # Extract re-exported symbols and source modules
            key_exports, dependencies = self._extract_barrel_info(
                content, language, heuristic_metadata
            )
            
            # Build purpose string with module info
            export_count = len(key_exports)
            module_names = self._format_module_list(dependencies)
            
            if module_names:
                purpose = f"Re-exports {export_count} symbols from submodules: {module_names}"
            else:
                purpose = f"Re-exports {export_count} symbols from submodules"
                
        elif reason == "type_definitions":
            # Try to infer domain from file path or interface names
            inferred_domain = self._infer_domain_from_types(
                file_path, heuristic_metadata
            )
            domain = inferred_domain or "types"
            purpose = f"Type definitions for {domain}"
            
            # List interfaces/types as exports
            key_exports = [iface['name'] for iface in heuristic_metadata.interfaces]
            
        elif reason == "constants_only":
            # Extract constant names as key exports
            key_exports = self._extract_constant_names(content, language)
            
        elif reason == "generated_code":
            # Try to identify what generated this file
            generator = self._identify_generator(content)
            if generator:
                purpose = f"Auto-generated code by {generator} (do not edit manually)"
            
        elif reason == "empty_module":
            # Keep defaults - nothing to extract
            pass
        
        response_time_ms = (time.time() - start_time) * 1000
        
        return FileSummary(
            file_path=file_path,
            language=language,
            purpose=purpose,
            pattern=pattern,
            key_exports=key_exports,
            dependencies=dependencies,
            domain=domain,
            model_used="template",
            tokens_used=0,
            response_time_ms=response_time_ms,
            is_skeleton=False,
            error=None,
            simple_file=True,
            simple_file_reason=reason
        )
    
    def _extract_barrel_info(
        self,
        content: str,
        language: str,
        heuristic_metadata: HeuristicMetadata
    ) -> Tuple[List[str], List[str]]:
        """
        Extract re-exported symbols and source modules from a barrel file.
        
        Returns:
            Tuple of (key_exports, dependencies/source_modules)
        """
        key_exports: List[str] = []
        source_modules: List[str] = []
        
        if language == 'python':
            # Parse Python __init__.py patterns
            # Pattern 1: from .module import symbol
            # Pattern 2: from .module import *
            # Pattern 3: __all__ = ['symbol1', 'symbol2']
            
            import re
            
            # Check __all__ first
            all_match = re.search(r'__all__\s*=\s*\[(.*?)\]', content, re.DOTALL)
            if all_match:
                all_str = all_match.group(1)
                # Extract quoted strings
                key_exports = re.findall(r'["\'](\w+)["\']', all_str)
            
            # Extract from imports
            from_imports = re.findall(
                r'from\s+\.(\w+)\s+import\s+(.+?)(?:\n|$)', 
                content
            )
            for module, imports in from_imports:
                source_modules.append(module)
                if imports.strip() != '*':
                    # Split imports and clean
                    for imp in imports.split(','):
                        imp = imp.strip()
                        # Handle "as" aliases
                        if ' as ' in imp:
                            imp = imp.split(' as ')[0].strip()
                        if imp and imp not in key_exports:
                            key_exports.append(imp)
            
            # Also check for direct imports
            direct_imports = re.findall(
                r'^import\s+\.(\w+)',
                content,
                re.MULTILINE
            )
            source_modules.extend(direct_imports)
            
        elif language in ['typescript', 'javascript']:
            # Parse TypeScript/JavaScript export patterns
            # Pattern 1: export { symbol } from './module'
            # Pattern 2: export * from './module'
            # Pattern 3: export { symbol }
            
            import re
            
            # Named re-exports
            reexport_matches = re.findall(
                r'export\s*\{([^}]+)\}\s*from\s*["\']\.?/?([^"\']+)["\']',
                content
            )
            for symbols, module in reexport_matches:
                source_modules.append(module)
                for sym in symbols.split(','):
                    sym = sym.strip()
                    # Handle "as" aliases
                    if ' as ' in sym:
                        sym = sym.split(' as ')[-1].strip()
                    if sym and sym not in key_exports:
                        key_exports.append(sym)
            
            # Wildcard re-exports
            wildcard_matches = re.findall(
                r'export\s*\*\s*from\s*["\']\.?/?([^"\']+)["\']',
                content
            )
            source_modules.extend(wildcard_matches)
            
            # Default re-exports
            default_matches = re.findall(
                r'export\s*\{\s*default\s*(?:as\s+(\w+))?\s*\}\s*from\s*["\']\.?/?([^"\']+)["\']',
                content
            )
            for alias, module in default_matches:
                source_modules.append(module)
                if alias:
                    key_exports.append(alias)
        
        # Use heuristic metadata exports if we didn't find any
        if not key_exports and heuristic_metadata.exports:
            key_exports = heuristic_metadata.exports
        
        # Use heuristic metadata imports as fallback for source modules
        if not source_modules and heuristic_metadata.imports:
            for imp in heuristic_metadata.imports:
                module = imp.get('module', '')
                if module and module.startswith('.'):
                    # Relative import
                    source_modules.append(module.lstrip('.'))
        
        # Deduplicate
        key_exports = list(dict.fromkeys(key_exports))
        source_modules = list(dict.fromkeys(source_modules))
        
        return key_exports, source_modules
    
    def _format_module_list(self, modules: List[str]) -> str:
        """Format a list of module names for display in purpose string."""
        if not modules:
            return ""
        
        # Clean up module names (remove paths, extensions)
        clean_modules = []
        for mod in modules:
            # Remove path prefix and extension
            name = Path(mod).stem if '/' in mod or '\\' in mod else mod
            name = name.lstrip('.')
            if name and name not in clean_modules:
                clean_modules.append(name)
        
        if len(clean_modules) == 0:
            return ""
        elif len(clean_modules) <= 3:
            return ", ".join(clean_modules)
        else:
            return f"{', '.join(clean_modules[:3])} and {len(clean_modules) - 3} more"
    
    def _infer_domain_from_types(
        self,
        file_path: str,
        heuristic_metadata: HeuristicMetadata
    ) -> Optional[str]:
        """Infer domain from file path or interface names."""
        # Common domain keywords in paths
        domain_keywords = {
            'auth': 'authentication',
            'user': 'user-management',
            'payment': 'payments',
            'order': 'orders',
            'product': 'products',
            'api': 'api',
            'dto': 'data-transfer',
            'model': 'data-models',
            'entity': 'entities',
            'response': 'api-responses',
            'request': 'api-requests',
            'config': 'configuration',
            'util': 'utilities',
            'common': 'common',
            'shared': 'shared',
            'core': 'core',
        }
        
        # Check file path
        path_lower = file_path.lower()
        for keyword, domain in domain_keywords.items():
            if keyword in path_lower:
                return domain
        
        # Check interface names
        if heuristic_metadata.interfaces:
            first_interface = heuristic_metadata.interfaces[0]['name'].lower()
            for keyword, domain in domain_keywords.items():
                if keyword in first_interface:
                    return domain
        
        return None
    
    def _extract_constant_names(self, content: str, language: str) -> List[str]:
        """Extract constant/variable names from a constants file."""
        import re
        constants = []
        
        if language == 'python':
            # Python: CONSTANT_NAME = value (uppercase by convention)
            matches = re.findall(r'^([A-Z][A-Z0-9_]*)\s*=', content, re.MULTILINE)
            constants = matches
            
        elif language in ['typescript', 'javascript']:
            # JS/TS: export const CONSTANT_NAME = value
            matches = re.findall(
                r'(?:export\s+)?const\s+([A-Z][A-Z0-9_]*)\s*=', 
                content, 
                re.MULTILINE
            )
            constants = matches
            
        elif language == 'java':
            # Java: public static final TYPE CONSTANT_NAME = value
            matches = re.findall(
                r'(?:public\s+)?(?:static\s+)?(?:final\s+)\w+\s+([A-Z][A-Z0-9_]*)\s*=',
                content,
                re.MULTILINE
            )
            constants = matches
        
        return constants[:10]  # Limit to 10 constants
    
    def _identify_generator(self, content: str) -> Optional[str]:
        """Try to identify what tool/framework generated this file."""
        import re
        
        # Common generator signatures
        generators = {
            r'protobuf': 'Protocol Buffers',
            r'openapi': 'OpenAPI Generator',
            r'swagger': 'Swagger Codegen',
            r'graphql': 'GraphQL Codegen',
            r'prisma': 'Prisma',
            r'sqlc': 'sqlc',
            r'grpc': 'gRPC',
            r'thrift': 'Apache Thrift',
            r'avro': 'Apache Avro',
            r'jooq': 'jOOQ',
            r'mybatis': 'MyBatis Generator',
            r'antlr': 'ANTLR',
            r'pydantic': 'Pydantic',
            r'dataclass_json': 'dataclass-json',
        }
        
        header = content[:500].lower()
        for pattern, name in generators.items():
            if re.search(pattern, header, re.IGNORECASE):
                return name
        
        return None
    
    def _extract_skeleton(
        self, 
        content: str, 
        language: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> str:
        """
        Extract skeleton from large file (signatures, docstrings, no implementation).
        
        Args:
            content: Full file content
            language: Programming language
            heuristic_metadata: Optional heuristic metadata
            
        Returns:
            Skeleton content with signatures and documentation only
        """
        try:
            if heuristic_metadata:
                return self._extract_skeleton_from_heuristics(content, heuristic_metadata)
            else:
                return self._extract_skeleton_fallback(content, language)
        except Exception as e:
            logger.debug(f"Skeleton extraction failed, using truncated content: {e}")
            # Fallback: return first portion of file
            lines = content.split('\n')
            return '\n'.join(lines[:self.config.max_file_lines // 2])
    
    def _extract_skeleton_from_heuristics(
        self, 
        content: str, 
        heuristic_metadata: HeuristicMetadata
    ) -> str:
        """Extract skeleton using heuristic metadata."""
        lines = content.split('\n')
        skeleton_lines = []
        included_ranges = set()
        
        # Include file-level docstring/comments at the top
        for i, line in enumerate(lines[:20]):  # Check first 20 lines
            stripped = line.strip()
            if stripped.startswith('"""') or stripped.startswith("'''") or stripped.startswith('/*'):
                skeleton_lines.append(line)
            elif stripped.startswith('#') or stripped.startswith('//'):
                skeleton_lines.append(line)
            elif stripped and not stripped.startswith(('import', 'from', 'package')):
                break
        
        # Include imports
        for import_info in heuristic_metadata.imports:
            # Find import lines in content (rough matching)
            import_stmt = import_info.get('statement', '')
            for i, line in enumerate(lines):
                if import_stmt in line and i not in included_ranges:
                    skeleton_lines.append(line)
                    included_ranges.add(i)
                    break
        
        # Include class signatures and docstrings
        for class_info in heuristic_metadata.classes:
            start_line = class_info['start_line'] - 1  # Convert to 0-based
            end_line = min(class_info['end_line'], len(lines))
            
            # Include class signature
            if start_line < len(lines):
                skeleton_lines.append(lines[start_line])
                included_ranges.add(start_line)
                
                # Look for docstring in next few lines
                for i in range(start_line + 1, min(start_line + 10, end_line)):
                    line = lines[i].strip()
                    if line.startswith('"""') or line.startswith("'''") or line.startswith('/*'):
                        skeleton_lines.append(lines[i])
                        included_ranges.add(i)
                        # Include until closing docstring
                        for j in range(i + 1, min(i + 20, end_line)):
                            skeleton_lines.append(lines[j])
                            included_ranges.add(j)
                            if '"""' in lines[j] or "'''" in lines[j] or '*/' in lines[j]:
                                break
                        break
        
        # Include function/method signatures
        for func_info in heuristic_metadata.functions + heuristic_metadata.methods:
            start_line = func_info['start_line'] - 1  # Convert to 0-based
            
            if start_line < len(lines) and start_line not in included_ranges:
                # Include function signature (might span multiple lines)
                signature_lines = []
                for i in range(start_line, min(start_line + 5, len(lines))):
                    signature_lines.append(lines[i])
                    if ':' in lines[i] or '{' in lines[i] or ';' in lines[i]:
                        break
                
                skeleton_lines.extend(signature_lines)
                for i in range(start_line, start_line + len(signature_lines)):
                    included_ranges.add(i)
        
        # Include docstrings
        for doc_info in heuristic_metadata.docstrings:
            doc_line = doc_info['line'] - 1  # Convert to 0-based
            if doc_line < len(lines) and doc_line not in included_ranges:
                skeleton_lines.append(lines[doc_line])
                included_ranges.add(doc_line)
        
        return '\n'.join(skeleton_lines)
    
    def _extract_skeleton_fallback(self, content: str, language: str) -> str:
        """Fallback skeleton extraction using regex patterns."""
        lines = content.split('\n')
        skeleton_lines = []
        
        # Language-specific patterns for signatures
        if language == 'python':
            patterns = [
                r'^\s*(class|def|async def)\s+\w+.*:',  # Class and function definitions
                r'^\s*""".*?"""',  # Docstrings
                r'^\s*#.*',  # Comments
                r'^\s*(import|from)\s+.*',  # Imports
            ]
        elif language == 'java':
            patterns = [
                r'^\s*(public|private|protected).*\{?\s*$',  # Method/class signatures
                r'^\s*/\*\*.*?\*/',  # Javadoc
                r'^\s*@\w+.*',  # Annotations
                r'^\s*import\s+.*',  # Imports
                r'^\s*package\s+.*',  # Package declaration
            ]
        elif language == 'javascript' or language == 'typescript':
            patterns = [
                r'^\s*(function|class|interface|type)\s+\w+.*',  # Declarations
                r'^\s*/\*\*.*?\*/',  # JSDoc
                r'^\s*import\s+.*',  # Imports
                r'^\s*export\s+.*',  # Exports
            ]
        else:
            # Generic patterns
            patterns = [
                r'^\s*(class|function|def|func|sub|procedure)\s+\w+.*',
                r'^\s*/\*.*?\*/',  # Block comments
                r'^\s*//.*',  # Line comments
                r'^\s*#.*',  # Hash comments
            ]
        
        # Extract lines matching patterns
        for line in lines:
            for pattern in patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    skeleton_lines.append(line)
                    break
        
        # If skeleton is too small, include some context
        if len(skeleton_lines) < 10:
            skeleton_lines.extend(lines[:50])  # Include first 50 lines
        
        return '\n'.join(skeleton_lines)
    
    async def _generate_llm_summary(
        self, 
        file_path: str, 
        language: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> Dict[str, Any]:
        """Generate summary using LLM."""
        
        # Build system prompt
        system_prompt = self._build_system_prompt()
        
        # Build user prompt
        user_prompt = self._build_user_prompt(file_path, language, content, heuristic_metadata)
        
        # Call LLM
        response = await self.llm_client.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=self.config.temperature,
            max_tokens=self.config.max_response_tokens
        )
        
        # Parse JSON response
        try:
            summary_data = json.loads(response.content)
            
            # Add response metadata
            summary_data['model_used'] = response.model
            summary_data['tokens_used'] = response.tokens_used
            summary_data['response_time_ms'] = response.response_time_ms
            
            return summary_data
            
        except json.JSONDecodeError as e:
            # Only warn if we actually got content back
            if response.content and response.content.strip():
                logger.debug(f"Failed to parse LLM response as JSON: {e}")
                logger.debug(f"Raw response: {response.content[:500]}...")
            else:
                logger.debug(f"LLM returned empty response for {file_path}")
            
            # Fallback: extract information from text response
            return self._parse_text_response(response, file_path, language)
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for LLM, conditional on config options."""
        base_prompt = """You are a code analysis expert. Analyze the provided code file and generate a structured summary in JSON format.

Focus on:
- The main purpose and functionality of the file
- Architectural patterns used (e.g., Repository, Controller, Service, Utility, Model, etc.)
- Key public APIs, classes, and functions that other code would use
- Important external dependencies
- The business domain or technical area this code belongs to"""
        
        # Add Phase 2 how_it_works instructions when enabled
        if self.config.include_how_it_works:
            base_prompt += """

IMPORTANT: Focus on HOW the code works, not just what it does.

For each significant method, explain:
- What data structures/indices it uses
- Any caching, memoization, or optimization strategies
- How parameters flow through the logic
- Any coordinate systems or index translations"""
        
        # Add implementation signals instructions when enabled
        if self.config.include_implementation_signals:
            base_prompt += """

Use the implementation signals provided (calls, attribute access, subscripts) to inform your analysis. These signals show you exactly what methods call, what data they access, and how they use parameters."""
        
        base_prompt += """

Respond with valid JSON only, no additional text or markdown formatting."""
        
        return base_prompt
    
    def _build_user_prompt(
        self, 
        file_path: str, 
        language: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> str:
        """Build user prompt for LLM, conditional on config options."""
        
        # Build JSON schema based on enabled features
        json_schema_parts = [
            f"Respond with JSON only:",
            f"{{",
            f'  "purpose": "1-2 sentence description of what this file does",',
            f'  "pattern": "architectural pattern (e.g., Repository, ViewModel, Controller, Utility)",',
            f'  "key_exports": ["list", "of", "main", "public", "APIs"],',
            f'  "dependencies": ["key", "external", "dependencies"],',
            f'  "domain": "business domain (e.g., authentication, payments, ui)"',
        ]
        
        # Add how_it_works fields when enabled
        if self.config.include_how_it_works:
            # Add comma to previous line
            json_schema_parts[-1] = json_schema_parts[-1] + ','
            json_schema_parts.extend([
                f'',
                f'  "how_it_works": "Explanation of HOW the code works - mechanisms, data flow, key algorithms, optimizations",',
                f'',
                f'  "key_mechanisms": [',
                f'    "list key patterns/mechanisms used (e.g., caching, lazy loading, window-relative indexing)"',
                f'  ]',
            ])
        
        # Add method_summaries when enabled
        if self.config.include_method_summaries:
            # Add comma to previous line
            json_schema_parts[-1] = json_schema_parts[-1] + ','
            json_schema_parts.extend([
                f'',
                f'  "method_summaries": {{',
                f'    "method_name": "what this method does and HOW - include data access patterns, parameter usage, notable logic"',
                f'  }}',
            ])
        
        json_schema_parts.append(f"}}")
        
        prompt_parts = [
            f"Analyze this {language} file and provide a structured summary.",
            f"",
            f"File: {file_path}",
            f"```{language}",
            content,
            f"```",
            f"",
        ]
        prompt_parts.extend(json_schema_parts)
        
        # Add heuristic context if available
        if heuristic_metadata:
            heuristic_context = []
            if heuristic_metadata.classes:
                class_names = [cls['name'] for cls in heuristic_metadata.classes]
                heuristic_context.append(f"Classes: {', '.join(class_names)}")
            
            if heuristic_metadata.functions:
                func_names = [func['name'] for func in heuristic_metadata.functions[:5]]  # Limit to 5
                heuristic_context.append(f"Functions: {', '.join(func_names)}")
            
            if heuristic_metadata.annotations:
                heuristic_context.append(f"Annotations: {', '.join(heuristic_metadata.annotations[:3])}")
            
            if heuristic_context:
                # Insert before the JSON schema (before "Respond with JSON only:")
                insert_idx = len(prompt_parts) - len(json_schema_parts)
                prompt_parts.insert(insert_idx, f"Context from static analysis: {'; '.join(heuristic_context)}")
                prompt_parts.insert(insert_idx + 1, "")
            
            # Add implementation signals from method details (Phase 2 enhancement)
            # Only when include_implementation_signals is enabled
            if self.config.include_implementation_signals and heuristic_metadata.method_details:
                signals_section = self._format_implementation_signals(heuristic_metadata.method_details)
                if signals_section:
                    # Insert before the JSON schema
                    insert_idx = len(prompt_parts) - len(json_schema_parts)
                    prompt_parts.insert(insert_idx, signals_section)
                    prompt_parts.insert(insert_idx + 1, "")
        
        return '\n'.join(prompt_parts)
    
    def _format_implementation_signals(self, method_details: List) -> str:
        """
        Format implementation signals from method details for LLM context.
        
        Provides the LLM with extracted signals about what methods call,
        what data they access, and how they use parameters.
        """
        if not method_details:
            return ""
        
        lines = ["Implementation Signals (from static analysis):"]
        
        # Limit to most significant methods to control token usage
        # Prioritize methods with more signals (likely more complex/important)
        sorted_details = sorted(
            method_details,
            key=lambda m: (
                len(m.internal_calls) + len(m.external_calls) +
                len(m.subscript_access) + len(m.attribute_reads)
            ),
            reverse=True
        )[:10]  # Top 10 most signal-rich methods
        
        for detail in sorted_details:
            method_signals = []
            
            # Combine internal and external calls
            all_calls = detail.internal_calls + detail.external_calls
            if all_calls:
                method_signals.append(f"calls: {', '.join(all_calls[:8])}")
            
            if detail.subscript_access:
                method_signals.append(f"subscripts: {', '.join(detail.subscript_access[:5])}")
            
            if detail.attribute_reads:
                method_signals.append(f"reads: {', '.join(detail.attribute_reads[:5])}")
            
            if detail.attribute_writes:
                method_signals.append(f"writes: {', '.join(detail.attribute_writes[:3])}")
            
            if detail.parameters_used:
                method_signals.append(f"uses params: {', '.join(detail.parameters_used)}")
            
            # Add structural hints for complex methods
            structural = []
            if detail.has_loop:
                structural.append("loops")
            if detail.has_conditional:
                structural.append("conditionals")
            if detail.has_try_except:
                structural.append("error handling")
            if detail.is_async:
                structural.append("async")
            if structural:
                method_signals.append(f"structure: {', '.join(structural)}")
            
            if method_signals:
                lines.append(f"  {detail.name}(): {'; '.join(method_signals)}")
        
        # Only return if we have actual signals
        if len(lines) > 1:
            return '\n'.join(lines)
        return ""
    
    def _parse_text_response(
        self, 
        response: LLMResponse, 
        file_path: str, 
        language: str
    ) -> Dict[str, Any]:
        """Parse non-JSON text response as fallback."""
        content = response.content
        
        # Try to extract JSON from response if it's wrapped in markdown or other text
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        # Fallback: create basic summary from text
        return {
            'purpose': f"Analysis of {Path(file_path).name}",
            'pattern': 'Unknown',
            'key_exports': [],
            'dependencies': [],
            'domain': language,
            'model_used': response.model,
            'tokens_used': response.tokens_used,
            'response_time_ms': response.response_time_ms,
            'error': 'Failed to parse JSON response'
        }