"""
File summarizer for background LLM summarization.

Handles file analysis, skeleton extraction for large files, and LLM prompt generation.
"""

import json
import logging
import re
from typing import Dict, Any, Optional, List, Union
from dataclasses import dataclass
from pathlib import Path

from .base import LLMClient, LLMResponse, LLMError
from ..search.heuristics import HeuristicMetadata
from ..config.summarization import SummarizationConfig

logger = logging.getLogger(__name__)


@dataclass
class FileSummary:
    """Summary of a file generated by LLM."""
    file_path: str
    language: str
    purpose: str
    pattern: str
    key_exports: List[str]
    dependencies: List[str]
    domain: str
    model_used: str
    tokens_used: Optional[int] = None
    response_time_ms: Optional[float] = None
    is_skeleton: bool = False
    error: Optional[str] = None
    # Phase 2: Implementation-aware summary fields
    how_it_works: Optional[str] = None  # Explanation of HOW the code works
    key_mechanisms: Optional[List[str]] = None  # Key mechanisms/patterns used
    method_summaries: Optional[Dict[str, str]] = None  # Per-method summaries
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for storage."""
        result = {
            'file_path': self.file_path,
            'language': self.language,
            'purpose': self.purpose,
            'pattern': self.pattern,
            'key_exports': self.key_exports,
            'dependencies': self.dependencies,
            'domain': self.domain,
            'model_used': self.model_used,
            'tokens_used': self.tokens_used,
            'response_time_ms': self.response_time_ms,
            'is_skeleton': self.is_skeleton,
            'error': self.error
        }
        # Include implementation-aware fields when present
        if self.how_it_works is not None:
            result['how_it_works'] = self.how_it_works
        if self.key_mechanisms is not None:
            result['key_mechanisms'] = self.key_mechanisms
        if self.method_summaries is not None:
            result['method_summaries'] = self.method_summaries
        return result


class FileSummarizer:
    """Summarizes files using LLM with skeleton extraction for large files."""
    
    def __init__(self, llm_client: LLMClient, config: SummarizationConfig):
        """
        Initialize file summarizer.
        
        Args:
            llm_client: LLM client to use for summarization
            config: Summarization configuration
        """
        self.llm_client = llm_client
        self.config = config
    
    async def summarize_file(
        self, 
        file_path: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata] = None
    ) -> FileSummary:
        """
        Summarize a file using LLM.
        
        Args:
            file_path: Path to the file
            content: File content
            heuristic_metadata: Optional heuristic metadata for context
            
        Returns:
            FileSummary with LLM-generated summary
        """
        try:
            # Determine language
            language = self._detect_language(file_path, heuristic_metadata)
            
            # Check if file should be skipped
            if self._should_skip_file(file_path):
                return FileSummary(
                    file_path=file_path,
                    language=language,
                    purpose="Skipped file (matches skip pattern)",
                    pattern="Skipped",
                    key_exports=[],
                    dependencies=[],
                    domain="skipped",
                    model_used="none",
                    error="File skipped due to skip pattern"
                )
            
            # Determine if we need to extract skeleton
            lines = content.split('\n')
            estimated_tokens = len(content.split())  # Rough token estimate
            
            use_skeleton = (
                len(lines) > self.config.max_file_lines or 
                estimated_tokens > self.config.max_file_tokens
            )
            
            # Prepare content for LLM
            if use_skeleton:
                llm_content = self._extract_skeleton(content, language, heuristic_metadata)
            else:
                llm_content = content
            
            # Generate summary using LLM
            summary_data = await self._generate_llm_summary(
                file_path, language, llm_content, heuristic_metadata
            )
            
            return FileSummary(
                file_path=file_path,
                language=language,
                purpose=summary_data.get('purpose', 'Unknown purpose'),
                pattern=summary_data.get('pattern', 'Unknown'),
                key_exports=summary_data.get('key_exports', []),
                dependencies=summary_data.get('dependencies', []),
                domain=summary_data.get('domain', 'unknown'),
                model_used=summary_data.get('model_used', 'unknown'),
                tokens_used=summary_data.get('tokens_used'),
                response_time_ms=summary_data.get('response_time_ms'),
                is_skeleton=use_skeleton,
                # Phase 2: Implementation-aware fields (optional, populated when LLM provides them)
                how_it_works=summary_data.get('how_it_works'),
                key_mechanisms=summary_data.get('key_mechanisms'),
                method_summaries=summary_data.get('method_summaries')
            )
            
        except Exception as e:
            logger.warning(f"Failed to summarize {file_path}: {e}")
            return FileSummary(
                file_path=file_path,
                language=self._detect_language(file_path, heuristic_metadata),
                purpose="Failed to generate summary",
                pattern="Error",
                key_exports=[],
                dependencies=[],
                domain="error",
                model_used="none",
                error=str(e)
            )
    
    def _detect_language(self, file_path: str, heuristic_metadata: Optional[HeuristicMetadata]) -> str:
        """Detect programming language from file path or metadata."""
        if heuristic_metadata:
            return heuristic_metadata.language
        
        # Fallback to file extension
        ext = Path(file_path).suffix.lower()
        ext_to_lang = {
            '.py': 'python',
            '.java': 'java',
            '.kt': 'kotlin',
            '.go': 'go',
            '.cs': 'csharp',
            '.swift': 'swift',
            '.m': 'objc',
            '.rb': 'ruby',
            '.c': 'c',
            '.h': 'c',
            '.cpp': 'cpp',
            '.hpp': 'cpp'
        }
        return ext_to_lang.get(ext, 'unknown')
    
    def _should_skip_file(self, file_path: str) -> bool:
        """Check if file should be skipped based on patterns."""
        import fnmatch
        
        for pattern in self.config.skip_patterns:
            if fnmatch.fnmatch(file_path, pattern):
                return True
        return False
    
    def _extract_skeleton(
        self, 
        content: str, 
        language: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> str:
        """
        Extract skeleton from large file (signatures, docstrings, no implementation).
        
        Args:
            content: Full file content
            language: Programming language
            heuristic_metadata: Optional heuristic metadata
            
        Returns:
            Skeleton content with signatures and documentation only
        """
        try:
            if heuristic_metadata:
                return self._extract_skeleton_from_heuristics(content, heuristic_metadata)
            else:
                return self._extract_skeleton_fallback(content, language)
        except Exception as e:
            logger.debug(f"Skeleton extraction failed, using truncated content: {e}")
            # Fallback: return first portion of file
            lines = content.split('\n')
            return '\n'.join(lines[:self.config.max_file_lines // 2])
    
    def _extract_skeleton_from_heuristics(
        self, 
        content: str, 
        heuristic_metadata: HeuristicMetadata
    ) -> str:
        """Extract skeleton using heuristic metadata."""
        lines = content.split('\n')
        skeleton_lines = []
        included_ranges = set()
        
        # Include file-level docstring/comments at the top
        for i, line in enumerate(lines[:20]):  # Check first 20 lines
            stripped = line.strip()
            if stripped.startswith('"""') or stripped.startswith("'''") or stripped.startswith('/*'):
                skeleton_lines.append(line)
            elif stripped.startswith('#') or stripped.startswith('//'):
                skeleton_lines.append(line)
            elif stripped and not stripped.startswith(('import', 'from', 'package')):
                break
        
        # Include imports
        for import_info in heuristic_metadata.imports:
            # Find import lines in content (rough matching)
            import_stmt = import_info.get('statement', '')
            for i, line in enumerate(lines):
                if import_stmt in line and i not in included_ranges:
                    skeleton_lines.append(line)
                    included_ranges.add(i)
                    break
        
        # Include class signatures and docstrings
        for class_info in heuristic_metadata.classes:
            start_line = class_info['start_line'] - 1  # Convert to 0-based
            end_line = min(class_info['end_line'], len(lines))
            
            # Include class signature
            if start_line < len(lines):
                skeleton_lines.append(lines[start_line])
                included_ranges.add(start_line)
                
                # Look for docstring in next few lines
                for i in range(start_line + 1, min(start_line + 10, end_line)):
                    line = lines[i].strip()
                    if line.startswith('"""') or line.startswith("'''") or line.startswith('/*'):
                        skeleton_lines.append(lines[i])
                        included_ranges.add(i)
                        # Include until closing docstring
                        for j in range(i + 1, min(i + 20, end_line)):
                            skeleton_lines.append(lines[j])
                            included_ranges.add(j)
                            if '"""' in lines[j] or "'''" in lines[j] or '*/' in lines[j]:
                                break
                        break
        
        # Include function/method signatures
        for func_info in heuristic_metadata.functions + heuristic_metadata.methods:
            start_line = func_info['start_line'] - 1  # Convert to 0-based
            
            if start_line < len(lines) and start_line not in included_ranges:
                # Include function signature (might span multiple lines)
                signature_lines = []
                for i in range(start_line, min(start_line + 5, len(lines))):
                    signature_lines.append(lines[i])
                    if ':' in lines[i] or '{' in lines[i] or ';' in lines[i]:
                        break
                
                skeleton_lines.extend(signature_lines)
                for i in range(start_line, start_line + len(signature_lines)):
                    included_ranges.add(i)
        
        # Include docstrings
        for doc_info in heuristic_metadata.docstrings:
            doc_line = doc_info['line'] - 1  # Convert to 0-based
            if doc_line < len(lines) and doc_line not in included_ranges:
                skeleton_lines.append(lines[doc_line])
                included_ranges.add(doc_line)
        
        return '\n'.join(skeleton_lines)
    
    def _extract_skeleton_fallback(self, content: str, language: str) -> str:
        """Fallback skeleton extraction using regex patterns."""
        lines = content.split('\n')
        skeleton_lines = []
        
        # Language-specific patterns for signatures
        if language == 'python':
            patterns = [
                r'^\s*(class|def|async def)\s+\w+.*:',  # Class and function definitions
                r'^\s*""".*?"""',  # Docstrings
                r'^\s*#.*',  # Comments
                r'^\s*(import|from)\s+.*',  # Imports
            ]
        elif language == 'java':
            patterns = [
                r'^\s*(public|private|protected).*\{?\s*$',  # Method/class signatures
                r'^\s*/\*\*.*?\*/',  # Javadoc
                r'^\s*@\w+.*',  # Annotations
                r'^\s*import\s+.*',  # Imports
                r'^\s*package\s+.*',  # Package declaration
            ]
        elif language == 'javascript' or language == 'typescript':
            patterns = [
                r'^\s*(function|class|interface|type)\s+\w+.*',  # Declarations
                r'^\s*/\*\*.*?\*/',  # JSDoc
                r'^\s*import\s+.*',  # Imports
                r'^\s*export\s+.*',  # Exports
            ]
        else:
            # Generic patterns
            patterns = [
                r'^\s*(class|function|def|func|sub|procedure)\s+\w+.*',
                r'^\s*/\*.*?\*/',  # Block comments
                r'^\s*//.*',  # Line comments
                r'^\s*#.*',  # Hash comments
            ]
        
        # Extract lines matching patterns
        for line in lines:
            for pattern in patterns:
                if re.match(pattern, line, re.IGNORECASE):
                    skeleton_lines.append(line)
                    break
        
        # If skeleton is too small, include some context
        if len(skeleton_lines) < 10:
            skeleton_lines.extend(lines[:50])  # Include first 50 lines
        
        return '\n'.join(skeleton_lines)
    
    async def _generate_llm_summary(
        self, 
        file_path: str, 
        language: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> Dict[str, Any]:
        """Generate summary using LLM."""
        
        # Build system prompt
        system_prompt = self._build_system_prompt()
        
        # Build user prompt
        user_prompt = self._build_user_prompt(file_path, language, content, heuristic_metadata)
        
        # Call LLM
        response = await self.llm_client.generate(
            prompt=user_prompt,
            system_prompt=system_prompt,
            temperature=self.config.temperature,
            max_tokens=self.config.max_response_tokens
        )
        
        # Parse JSON response
        try:
            summary_data = json.loads(response.content)
            
            # Add response metadata
            summary_data['model_used'] = response.model
            summary_data['tokens_used'] = response.tokens_used
            summary_data['response_time_ms'] = response.response_time_ms
            
            return summary_data
            
        except json.JSONDecodeError as e:
            # Only warn if we actually got content back
            if response.content and response.content.strip():
                logger.debug(f"Failed to parse LLM response as JSON: {e}")
                logger.debug(f"Raw response: {response.content[:500]}...")
            else:
                logger.debug(f"LLM returned empty response for {file_path}")
            
            # Fallback: extract information from text response
            return self._parse_text_response(response, file_path, language)
    
    def _build_system_prompt(self) -> str:
        """Build system prompt for LLM."""
        return """You are a code analysis expert. Analyze the provided code file and generate a structured summary in JSON format.

Focus on:
- The main purpose and functionality of the file
- Architectural patterns used (e.g., Repository, Controller, Service, Utility, Model, etc.)
- Key public APIs, classes, and functions that other code would use
- Important external dependencies
- The business domain or technical area this code belongs to

IMPORTANT: Focus on HOW the code works, not just what it does.

For each significant method, explain:
- What data structures/indices it uses
- Any caching, memoization, or optimization strategies
- How parameters flow through the logic
- Any coordinate systems or index translations

Use the implementation signals provided (calls, attribute access, subscripts) to inform your analysis. These signals show you exactly what methods call, what data they access, and how they use parameters.

Respond with valid JSON only, no additional text or markdown formatting."""
    
    def _build_user_prompt(
        self, 
        file_path: str, 
        language: str, 
        content: str, 
        heuristic_metadata: Optional[HeuristicMetadata]
    ) -> str:
        """Build user prompt for LLM."""
        
        prompt_parts = [
            f"Analyze this {language} file and provide a structured summary.",
            f"",
            f"File: {file_path}",
            f"```{language}",
            content,
            f"```",
            f"",
            f"Respond with JSON only:",
            f"{{",
            f'  "purpose": "1-2 sentence description of what this file does",',
            f'  "pattern": "architectural pattern (e.g., Repository, ViewModel, Controller, Utility)",',
            f'  "key_exports": ["list", "of", "main", "public", "APIs"],',
            f'  "dependencies": ["key", "external", "dependencies"],',
            f'  "domain": "business domain (e.g., authentication, payments, ui)",',
            f'',
            f'  "how_it_works": "Explanation of HOW the code works - mechanisms, data flow, key algorithms, optimizations",',
            f'',
            f'  "key_mechanisms": [',
            f'    "list key patterns/mechanisms used (e.g., caching, lazy loading, window-relative indexing)"',
            f'  ],',
            f'',
            f'  "method_summaries": {{',
            f'    "method_name": "what this method does and HOW - include data access patterns, parameter usage, notable logic"',
            f'  }}',
            f"}}"
        ]
        
        # Add heuristic context if available
        if heuristic_metadata:
            heuristic_context = []
            if heuristic_metadata.classes:
                class_names = [cls['name'] for cls in heuristic_metadata.classes]
                heuristic_context.append(f"Classes: {', '.join(class_names)}")
            
            if heuristic_metadata.functions:
                func_names = [func['name'] for func in heuristic_metadata.functions[:5]]  # Limit to 5
                heuristic_context.append(f"Functions: {', '.join(func_names)}")
            
            if heuristic_metadata.annotations:
                heuristic_context.append(f"Annotations: {', '.join(heuristic_metadata.annotations[:3])}")
            
            if heuristic_context:
                prompt_parts.insert(-8, f"Context from static analysis: {'; '.join(heuristic_context)}")
                prompt_parts.insert(-8, "")
            
            # Add implementation signals from method details (Phase 2 enhancement)
            if heuristic_metadata.method_details:
                signals_section = self._format_implementation_signals(heuristic_metadata.method_details)
                if signals_section:
                    prompt_parts.insert(-8, signals_section)
                    prompt_parts.insert(-8, "")
        
        return '\n'.join(prompt_parts)
    
    def _format_implementation_signals(self, method_details: List) -> str:
        """
        Format implementation signals from method details for LLM context.
        
        Provides the LLM with extracted signals about what methods call,
        what data they access, and how they use parameters.
        """
        if not method_details:
            return ""
        
        lines = ["Implementation Signals (from static analysis):"]
        
        # Limit to most significant methods to control token usage
        # Prioritize methods with more signals (likely more complex/important)
        sorted_details = sorted(
            method_details,
            key=lambda m: (
                len(m.internal_calls) + len(m.external_calls) +
                len(m.subscript_access) + len(m.attribute_reads)
            ),
            reverse=True
        )[:10]  # Top 10 most signal-rich methods
        
        for detail in sorted_details:
            method_signals = []
            
            # Combine internal and external calls
            all_calls = detail.internal_calls + detail.external_calls
            if all_calls:
                method_signals.append(f"calls: {', '.join(all_calls[:8])}")
            
            if detail.subscript_access:
                method_signals.append(f"subscripts: {', '.join(detail.subscript_access[:5])}")
            
            if detail.attribute_reads:
                method_signals.append(f"reads: {', '.join(detail.attribute_reads[:5])}")
            
            if detail.attribute_writes:
                method_signals.append(f"writes: {', '.join(detail.attribute_writes[:3])}")
            
            if detail.parameters_used:
                method_signals.append(f"uses params: {', '.join(detail.parameters_used)}")
            
            # Add structural hints for complex methods
            structural = []
            if detail.has_loop:
                structural.append("loops")
            if detail.has_conditional:
                structural.append("conditionals")
            if detail.has_try_except:
                structural.append("error handling")
            if detail.is_async:
                structural.append("async")
            if structural:
                method_signals.append(f"structure: {', '.join(structural)}")
            
            if method_signals:
                lines.append(f"  {detail.name}(): {'; '.join(method_signals)}")
        
        # Only return if we have actual signals
        if len(lines) > 1:
            return '\n'.join(lines)
        return ""
    
    def _parse_text_response(
        self, 
        response: LLMResponse, 
        file_path: str, 
        language: str
    ) -> Dict[str, Any]:
        """Parse non-JSON text response as fallback."""
        content = response.content
        
        # Try to extract JSON from response if it's wrapped in markdown or other text
        json_match = re.search(r'\{.*\}', content, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        
        # Fallback: create basic summary from text
        return {
            'purpose': f"Analysis of {Path(file_path).name}",
            'pattern': 'Unknown',
            'key_exports': [],
            'dependencies': [],
            'domain': language,
            'model_used': response.model,
            'tokens_used': response.tokens_used,
            'response_time_ms': response.response_time_ms,
            'error': 'Failed to parse JSON response'
        }